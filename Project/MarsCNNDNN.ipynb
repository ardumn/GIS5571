{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "085903e7-2aa3-4d68-a5fc-c1374afe7fef",
   "metadata": {},
   "source": [
    "# Mars Deep Neural Network for Classifying Geological Features\n",
    "### Advanced Geocomputing 5543\n",
    "\n",
    "> The synopsis of this project is to classify geological features on the surface of Mars to using training data from many subsets of Martian satellite imagery.  This project was originally intended to classify deepfake satellite imagery to draw out comparisons between EuroSAT and Planet Data.  Modeling off the Cheskape Bay Conservancy to find conservation efforts to minimize and reduce environmental degradation and maximize sustainability.  Transitioning to using a generative adversarial network (GAN) and deep learning, and template cat pet picture script generator, the script was altered to generate EurotSat data and now Mars geological subset images.\n",
    "\n",
    "> Alexander Danielson, (in collaboration with Jake Ford and Timothy Tran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41436244-cbfe-4dc7-b048-d6792b5c1754",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (526942852.py, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_91/526942852.py\"\u001b[0;36m, line \u001b[0;32m33\u001b[0m\n\u001b[0;31m    labels=\"inferred\",\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    " #Inspiration: https://www.tensorflow.org/tutorials/generative/dcgan\n",
    "# https://www.kaggle.com/code/joxcat/simple-cat-generator\n",
    "# Project worked on by Alexander Danielson, Jake Ford, and Timothy Tran\n",
    "# Import modules below\n",
    "# This script uses TensorFlow models to create deepfake satellite imagery data (orginal project)\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from IPython import display\n",
    "import os\n",
    "import PIL\n",
    "\n",
    "# Parameters to define\n",
    "# We can change how many epochs, examples to create, etc\n",
    "img_size = 64\n",
    "EPOCHS = 1\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 12\n",
    "BUFFER_SIZE = 40000\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# The strategy below is setting up parallel code for use with TensorFlow\n",
    "# It is useful the more GPUs we have access to\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Will need to be changed if used on a different machine, like MSI's Mesabi.\n",
    "mars_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"C:\\Users\\Alexander Danielson\\Desktop\\Fall 2022Spring2023\\ArcGIS I\\FinalProject\\mars\\train_images\", # THIS IS WHERE YOU CHANGE THE DIRECTORY\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\",\n",
    "    class_names=['rockfall'],\n",
    "    color_mode=\"rgb\", # output color\n",
    "    batch_size=32,\n",
    "    image_size=(img_size, img_size),\n",
    "    shuffle=True,\n",
    "    seed=None,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    ")\n",
    "\n",
    "\n",
    "# Creating arrays for our images and labels\n",
    "# To later be formatted into a numpy array\n",
    "mars_train_labels = []\n",
    "mars_train_images = []\n",
    "\n",
    "for images, labels in eu_dataset:\n",
    "    for i in range(len(images)):\n",
    "      eu_train_images.append(images[i])\n",
    "      eu_train_labels.append(labels[i])\n",
    "      \n",
    "# Formatting our dataset into a numpy array\n",
    "# It has a dimension of 3 because we are working with RGB bands\n",
    "# The print statements were helpful to keep track of the shape of the data\n",
    "eu_images = np.array(eu_train_images)\n",
    "print(eu_images.shape)\n",
    "eu_images = eu_images.reshape(eu_images.shape[0],img_size,img_size,3)\n",
    "print(eu_images.shape)\n",
    "\n",
    "eu_labels = np.array(eu_train_labels)\n",
    "print(eu_labels.shape)\n",
    "eu_labels = eu_labels.reshape(eu_labels.shape[0],)\n",
    "print(eu_labels.shape)\n",
    "\n",
    "# Uses save from numpy to keep track of the images and labels\n",
    "from numpy import save\n",
    "save('images.npy', eu_images)\n",
    "save('labels.npy', eu_labels)\n",
    "\n",
    "\n",
    "train_images = eu_images\n",
    "train_labels = eu_labels\n",
    "\n",
    "train_images = train_images.reshape(train_images.shape[0], img_size, img_size, 3).astype('float32')\n",
    "train_images = (train_images - 127.5) / 127.5 # Normalize the images to [-1, 1]\n",
    "\n",
    "# Define the train dataset model here by slicing the train images and shuffling them\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "# Generator model from TensorFlow GAN tutorial that we are using\n",
    "# Parameters were changed, like the input size and filters, so the model\n",
    "# will correctly take our input\n",
    "# Added strategy.scope() as parallel code\n",
    "with strategy.scope():\n",
    "    def make_generator_model():\n",
    "\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(layers.Dense(8*8*256, use_bias=False, input_shape=(100,)))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.LeakyReLU())\n",
    "\n",
    "        model.add(layers.Reshape((8, 8, 256)))\n",
    "        assert model.output_shape == (None, 8, 8, 256) # Note: None is the batch size\n",
    "\n",
    "        model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "        assert model.output_shape == (None, 8, 8, 128)\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.LeakyReLU())\n",
    "\n",
    "        model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "        assert model.output_shape == (None, 16, 16, 64)\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.LeakyReLU())\n",
    "\n",
    "        model.add(layers.Conv2DTranspose(3, (10, 10), strides=(4, 4), padding='same', use_bias=False, activation='tanh'))\n",
    "        assert model.output_shape == (None, 64, 64, 3) #generator model\n",
    "\n",
    "        return model\n",
    "\n",
    "# Setting the model into a variable so it will be used throughout the code\n",
    "generator = make_generator_model()\n",
    "\n",
    "# This shows us what the model is creating everytime we run it\n",
    "# Makes the noise random so it shows us what its creating\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "# The code below just shows us what the generated image looks like, but we\n",
    "# don't need it in our case.\n",
    "#plt.imshow(generated_image[0, :, :, 0], cmap='gray')\n",
    "\n",
    "# This is the discriminator model, derived from the inspiration listed \n",
    "# above. Some parameters had to be changed in order to take our eurosat input\n",
    "with strategy.scope():\n",
    "    def make_discriminator_model():\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(layers.Conv2D(64, (10, 10), strides=(2, 2), padding='same',\n",
    "                                         input_shape=[64, 64, 3]))\n",
    "        model.add(layers.LeakyReLU())\n",
    "        model.add(layers.Dropout(0.3))\n",
    "\n",
    "        model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "        model.add(layers.LeakyReLU())\n",
    "        model.add(layers.Dropout(0.3))\n",
    "\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(1))\n",
    "\n",
    "        return model\n",
    "\n",
    "# Creating the discriminator model\n",
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print(decision)\n",
    "\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "# We are able to save checkpoints so that can pick up right where we left off\n",
    "# It saves every 15 epochs and can be called later on in case it gets interrupted\n",
    "# Please make sure that the directory is the same as the dataset if run elsewhere\n",
    "checkpoint_dir = '../cat_gan' # Should be the same directory as the dataset\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n",
    "\n",
    "\n",
    "# Creates the random seed to generate\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(noise, training=True)\n",
    "\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "\n",
    "# The actual function that takes out dataset and epochs to train our model\n",
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        \n",
    "        for image_batch in dataset:\n",
    "            train_step(image_batch)\n",
    "\n",
    "    # This saves the image at each epoch for later reference\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator,\n",
    "                             epoch + 1,\n",
    "                             seed)\n",
    "\n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        \n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "# Generate after the final epoch\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           seed)\n",
    "  \n",
    "  \n",
    "# A function created to save the images that the model outputs into the root\n",
    "# directory of where the program is stored. \n",
    "  \n",
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  predictions = model(test_input, training=False)\n",
    "  \n",
    "  # This says it's never used, but it actually helps format the final pictures\n",
    "  # so that they don't look so far apart from each other\n",
    "  fig = plt.figure(figsize=(4,4))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "      plt.axis('off')\n",
    "\n",
    "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()\n",
    "\n",
    "# A function below to display created images at certain epochs\n",
    "def display_image(epoch):\n",
    "    return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "\n",
    "# A call to run the script with how many epochs we have set\n",
    "train(train_dataset, EPOCHS)\n",
    "\n",
    "# Restores a saved model, the checkpoint, with the following code\n",
    "#checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f5284a-9fc9-437e-86cf-76223aa2f799",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
